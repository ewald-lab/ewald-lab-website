---
title: ""
---

The big problem that motivates our research is that humans and ecosystems are constantly exposed to a cocktail of human created chemicals, and we have no idea what most of them do to biological systems. Additionally, many pharmaceuticals fail clinical trials due to toxicity not predicted by preclinical models, draining resources and increasing the costs for developing new therapeutics. Traditional toxicity testing methods are based on exposing animals to chemicals and observing high-level endpoints like mortality, impacts on reproduction, and incidence of tissue lesions and tumors. They cannot fill this information gap, for multiple reasons:

They are too expensive and time consuming to assess risk for large numbers of chemicals
They are ethically concerning - members of society are increasingly calling for the end of animal testing
They produce limited information on the molecular mechanisms of toxicity, which makes it extremely difficult to extrapolate risk assessments across different species and life stages and sexes

## Toxicity testing in the 21st century

For decades, toxicologists and regulators have been advocating for a paradigm shift in how chemical toxicity is assessed. Instead of relying on in vivo animal testing, a suite of in silico and in vitro models should be used to gain insights on how chemicals are distributed throughout biological systems upon exposure, how they interact with macromolecules to perturb cells, and to predict whether these exposures and perturbations could cause adverse health outcomes in whole organisms. The vision for this paradigm shift for human chemical risk assessment was articulated by the United States National Research Council in a book called ‘Toxicity Testing in the 21st Century’ ([free PDF](https://drive.google.com/file/d/1d84Nb0_hCw2c3gYbSxOqyXdKtz21gzn8/view?usp=sharing), concept abbreviated as TT21C). The entire 200-page book is quite accessible, and worth downloading even if just for the summary section (pages 1-17). There was also a short summary published as [a scientific article](https://academic.oup.com/toxsci/article/107/2/324/1683303), as well as an update on the [progress and future outlook](https://link.springer.com/article/10.1007/s00204-019-02613-4) published 10 years later.

## Adverse outcome pathways

Ecotoxicologists face an even more daunting challenge - they must assess the risk of chemical exposure to all species within a given ecosystem. At around the same time that the TT21C book was written, ecotoxicologists came up with the concept of “adverse outcome pathways” ([introductory conceptual paper](https://academic.oup.com/etc/article-abstract/29/3/730/7763337), abbreviation AOP). AOPs are a theoretical framework that link a molecular-level event caused by a chemical to a series of biological changes leading to an adverse health outcome. The main goal was to organise information such that molecular changes could be mechanistically linked to adverse outcomes that regulators cared about, providing a logical basis for assessing the risk of exposure to chemical mixtures and enabling extrapolation of biological responses across species.

While TT21C originated in the context of human health and AOPs in ecotoxicology, both concepts are now widely recognized as valuable tools for advancing understanding and assessment in both human and ecological health, with broad applicability across disciplines. Since 2009, much of the work in this area has been encompassed under the development of New Approach Methodologies (NAMs). NAMs are non-animal testing strategies that include in vitro assays, computational models, and high-throughput screening techniques to assess chemical safety more efficiently, ethically, and often yielding detailed mechanistic insights.

## Toxicogenomics data

Many NAMs are targeted in vitro assays that measure one thing, for example the activity of a single transcription factor or the activation of a particular cell death signaling pathway. While these are relatively easy to interpret, it is not logistically feasible to measure thousands of targeted assays simultaneously to capture all of biological space. Because of this, there is great interest in using cell profiling and omics data to measure hundreds to thousands of endpoints simultaneously. When omics data is collected within the context of toxicology, it is sometimes called ‘toxicogenomics data’. For introduction, read this great [review of toxicogenomics data for human health](https://www.nature.com/articles/s41576-024-00767-1), this [vision for basing chemical risk assessment on omics data](https://academic.oup.com/toxsci/article/190/2/127/6722637), and the US Environmental Protection Agency’s [blueprint for computational toxicology](https://academic.oup.com/toxsci/article/169/2/317/5369737), which explains how omics data fits into the big picture is here.

Other sources of information include scientific associations (we mainly engage with the Society for Environmental Toxicology and Chemistry, Eurotox, and the Society of Toxicology) and popular science books. Some of my favourite popular science books related to environmental toxicology are:

* “Silent Spring” by Rachel Carson (a classic)
* “Our Stolen Future” by Theo Colborn, Dianne Dumanoski, and John Peterson Meyers
* “To Dye For” by Alden Wicker
